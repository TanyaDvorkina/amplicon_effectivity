
Load prepared data
```{r}
prepared_data <- read.table("../data/features.tsv", sep = "\t", header=TRUE)
totalTable <- NULL
totalTable$sd <- prepared_data$sd
totalTable$mean <- prepared_data$mean
totalTableOfMeans <- NULL
totalTableOfMeans$sd <- prepared_data$MeansSD
#prepared_data$Ten <- as.numeric(prepared_data$Ten)
prepared_data$Start_Dangling_End <- as.numeric(prepared_data$Start_Dangling_End)
prepared_data$Stop_Dangling_End <- as.numeric(prepared_data$Stop_Dangling_End) 
prepared_data$Start_Dangling_End <- as.numeric(prepared_data$Fwd_END_3)
prepared_data$Stop_Dangling_End <- as.numeric(prepared_data$Rev_END_3) 

prepared_data_num <- prepared_data
prepared_data_num$Start_Dangling_End <- as.numeric(prepared_data_num$Start_Dangling_End)
prepared_data_num$Stop_Dangling_End <- as.numeric(prepared_data_num$Stop_Dangling_End)
prepared_data_num$Fwd_END_3 <- as.numeric(prepared_data_num$Fwd_END_3)
prepared_data_num$Rev_END_3 <- as.numeric(prepared_data_num$Rev_END_3)
prepared_data_num$res <- NULL
prepared_data_num$logres <- NULL
prepared_data_num$MeansSD <- NULL
print(ncol(prepared_data_num))
prepared_data.scale<- scale(prepared_data_num[2:ncol(prepared_data_num)], center=TRUE, scale=TRUE)
prepared_data[,names(prepared_data.scale)] <- prepared_data.scale
```


Prepare train and test sets to get information from rf
```{r}
library(randomForest)
splitdf <- function(dataframe, n = 4, seed=NULL) {
    if (!is.null(seed)) set.seed(seed)
    index <- 1:nrow(dataframe)
    trainindex <- sample(index, trunc(length(index)/n))
    trainset <- dataframe[trainindex, ]
    testset <- dataframe[-trainindex, ]
    list(trainset=trainset,testset=testset)
}
splited <- splitdf(prepared_data)
trainSet <- splited$trainset
testSet <- splited$testset
```

Use all features to train model for mean
```{r}
formula <- mean ~ Amplicon_Len + Amplicon_GC_content + Fwd_END_3 + Rev_END_3+Start_Dangling_End+Stop_Dangling_End + Fwd_Primer_Len +Fwd_Primer_GC_content+ Rev_Primer_Len +Rev_Primer_GC_content + Fwd_Primer_MaxPolyX + Rev_Primer_MaxPolyX + Amplicon_MaxPolyX + Fwd_Primer_Entropy + Rev_Primer_Entropy + Amplicon_Entropy + Fwd_Primer_Hash + Rev_Primer_Hash + Amplicon_Hash 
rf <- randomForest(formula, trainSet)
importance(rf)
fit <- lm(formula, data=trainSet)
#plot(fit)
#predicted <- predict(fit, newdata = testSet)
#rmse(predicted - testSet$mean)
#predicted <- predict(rf, newdata = testSet)
#rmse(predicted - testSet$mean)
```

Use all features to train model for sd
```{r}
formula <- sd ~ Amplicon_Len + Amplicon_GC_content + Fwd_END_3 + Rev_END_3+Start_Dangling_End+Stop_Dangling_End + Fwd_Primer_Len +Fwd_Primer_GC_content+ Rev_Primer_Len +Rev_Primer_GC_content + Fwd_Primer_MaxPolyX + Rev_Primer_MaxPolyX + Amplicon_MaxPolyX + Fwd_Primer_Entropy + Rev_Primer_Entropy + Amplicon_Entropy + Fwd_Primer_Hash + Rev_Primer_Hash + Amplicon_Hash 
rf <- randomForest(formula, trainSet)
importance(rf)
#fit <- lm(formula, data=trainSet)
#plot(fit)
#predicted <- predict(fit, newdata = testSet)
#rmse(predicted - testSet$sd)
#predicted <- predict(rf, newdata = testSet)
#rmse(predicted - testSet$sd)
```


Different feature selection tests
```{r}
library(lattice)
library(FSelector)
histogram(prepared_data$sd)
sd(prepared_data$sd)
formula <- sd ~ Amplicon_Len + Fwd_END_3 + Rev_END_3+Start_Dangling_End+Stop_Dangling_End + Fwd_Primer_Len + Rev_Primer_Len + Fwd_Primer_MaxPolyX + Rev_Primer_MaxPolyX + Amplicon_MaxPolyX + Fwd_Primer_Hash + Rev_Primer_Hash + Amplicon_Hash 
chi.squared(formula, data = prepared_data)
chi <- chi.squared(formula, data = prepared_data)
chi$names <- rownames(chi)
chi <- chi[order(-chi$attr_importance), ]
barchart( chi$names ~ chi$attr_importance, main = "Chi-squared test", xlab = "Feature improtance")
information.gain(formula, data = prepared_data)
gain.ratio(formula, data = prepared_data)
symmetrical.uncertainty(formula, data = prepared_data)
oneR(formula, data = prepared_data)
formula <- sd ~ Amplicon_Len + Amplicon_GC_content + Fwd_END_3 + Rev_END_3+Start_Dangling_End+Stop_Dangling_End + Fwd_Primer_Len +Fwd_Primer_GC_content+ Rev_Primer_Len +Rev_Primer_GC_content + Fwd_Primer_MaxPolyX + Rev_Primer_MaxPolyX + Amplicon_MaxPolyX + Fwd_Primer_Entropy + Rev_Primer_Entropy + Amplicon_Entropy + Fwd_Primer_Hash + Rev_Primer_Hash + Amplicon_Hash 
random.forest.importance(formula, data = prepared_data, importance.type = 1)
```

```{r}
library(lattice)
library(FSelector)
sd(prepared_data$MeansSD)
formula <- MeansSD ~ Amplicon_Len + Fwd_END_3 + Rev_END_3+Start_Dangling_End+Stop_Dangling_End + Fwd_Primer_Len + Rev_Primer_Len + Fwd_Primer_MaxPolyX + Rev_Primer_MaxPolyX + Amplicon_MaxPolyX + Fwd_Primer_Hash + Rev_Primer_Hash + Amplicon_Hash
chi.squared(formula, data = prepared_data)
chi <- chi.squared(formula, data = prepared_data)
chi$names <- rownames(chi)
chi <- chi[order(-chi$attr_importance), ]
barchart( chi$names ~ chi$attr_importance, main = "Chi-squared test", xlab = "Feature improtance")
information.gain(formula, data = prepared_data)
gain.ratio(formula, data = prepared_data)
symmetrical.uncertainty(formula, data = prepared_data)
oneR(formula, data = prepared_data)
formula <- MeansSD ~ Amplicon_Len + Amplicon_GC_content + Fwd_END_3 + Rev_END_3+Start_Dangling_End+Stop_Dangling_End  + Fwd_Primer_Len +Fwd_Primer_GC_content+ Rev_Primer_Len +Rev_Primer_GC_content + Fwd_Primer_MaxPolyX + Rev_Primer_MaxPolyX + Amplicon_MaxPolyX + Fwd_Primer_Entropy + Rev_Primer_Entropy + Amplicon_Entropy + Fwd_Primer_Hash + Rev_Primer_Hash + Amplicon_Hash 
random.forest.importance(formula, data = prepared_data, importance.type = 1)
```

```{r}
library(FSelector)
histogram(prepared_data$mean)
sd(prepared_data$mean)
formula <- mean ~ Amplicon_Len + Fwd_END_3 + Rev_END_3+Start_Dangling_End+Stop_Dangling_End + Fwd_Primer_Len + Rev_Primer_Len + Fwd_Primer_MaxPolyX + Rev_Primer_MaxPolyX + Amplicon_MaxPolyX + Fwd_Primer_Hash + Rev_Primer_Hash  + Amplicon_Hash
chi.squared(formula, data = prepared_data)
chi <- chi.squared(formula, data = prepared_data)
chi$names <- rownames(chi)
chi <- chi[order(-chi$attr_importance), ]
barchart( chi$names ~ chi$attr_importance, main = "Chi-squared test", xlab = "Feature improtance")
information.gain(formula, data = prepared_data)
gain.ratio(formula, data = prepared_data)
symmetrical.uncertainty(formula, data = prepared_data)
oneR(formula, data = prepared_data)
formula <- mean ~ Amplicon_Len + Amplicon_GC_content + Fwd_END_3 + Rev_END_3+Start_Dangling_End+Stop_Dangling_End + Fwd_Primer_Len +Fwd_Primer_GC_content+ Rev_Primer_Len +Rev_Primer_GC_content + Fwd_Primer_MaxPolyX + Rev_Primer_MaxPolyX + Amplicon_MaxPolyX + Fwd_Primer_Entropy + Rev_Primer_Entropy + Amplicon_Entropy + Fwd_Primer_Hash + Rev_Primer_Hash + Amplicon_Hash 
random.forest.importance(formula, data = prepared_data, importance.type = 1)
```

Feature subset selection tests
```{r}
library(corrplot)
prepared_data_num <- prepared_data
prepared_data_num$Start_Dangling_End <- as.numeric(prepared_data_num$Start_Dangling_End)
prepared_data_num$Stop_Dangling_End <- as.numeric(prepared_data_num$Stop_Dangling_End)
prepared_data_num$Fwd_END_3 <- as.numeric(prepared_data_num$Fwd_END_3)
prepared_data_num$Rev_END_3 <- as.numeric(prepared_data_num$Rev_END_3)
prepared_data_num$res <- NULL
prepared_data_num$logres <- NULL
prepared_data_num$MeansSD <- NULL
print(ncol(prepared_data_num))
prepared_data.scale<- scale(prepared_data_num[2:ncol(prepared_data_num)], center=TRUE, scale=TRUE)
cor_prepared_data <- cor(prepared_data.scale)
corrplot(cor_prepared_data, order = "original")
library(cvTools)
library(caret)
# control <- trainControl(method="repeatedcv", number=10, repeats=3)
# model <- train(res~., data=prepared_data, method="lvq", preProcess="scale", trControl=control)
# # estimate variable importance
# importance <- varImp(model, scale=FALSE)
# # summarize importance
# print(importance)
# # plot importance
# plot(importance)
evaluator <- function(subset) { 
    valid_names <- c(subset,"mean")
    dt <- prepared_data[,valid_names]
    rf <- lm(mean ~ ., dt)
    result <- cvFit(rf, data =  dt, y =  dt$mean, cost = rtmspe,
        K = 5, R = 10, costArgs = list(trim = 0.1), seed = 1234)
    print(valid_names)
    print(-result$cv)
    return(-result$cv)
}

#perform the best subset search
#subset <- exhaustive.search(names(prepared_data[-c(1, 11)]), evaluator)
#subset <- best.first.search(names(prepared_data[-c(1, 11)]), evaluator)

#MEAN
#"Amplicon_Len"      "Amplicon_MaxPolyX" "Amplicon_Entropy" 
#"Rev_Primer_Len"     "Amplicon_Len"       "Amplicon_MaxPolyX"  "Rev_Primer_Entropy" "Amplicon_Entropy"  
#"Amplicon_Len"      "Amplicon_MaxPolyX" "Amplicon_Entropy"  "Amplicon_Hash"   

#"Amplicon_Len"          "Fwd_Primer_GC_content" "Amplicon_MaxPolyX"     "Amplicon_Entropy"  
#"Amplicon_Len"          "Fwd_Primer_GC_content" "Amplicon_MaxPolyX"     "Amplicon_Entropy"  
#"Amplicon_Len"          "Fwd_Primer_GC_content" "Amplicon_Entropy" 



#SD

#"Amplicon_Len"     "Amplicon_Entropy" "Amplicon_Hash"  
#"Amplicon_Len"     "Amplicon_Entropy"
#"Amplicon_Len"     "Amplicon_Entropy" "Amplicon_Hash" 

#"Amplicon_MaxPolyX"
```

